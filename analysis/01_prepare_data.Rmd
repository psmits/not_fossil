# Preparing data for analysis

The next step is to process our data so that it is ready for analysis. This process takes a lot of steps, but once it is complete we can start visualizing and modeling out data.

I've already prepared some data for this analysis. Check out `00_download_scrap.Rmd` for that (annotated) script.

```{r import_raw, message = FALSE, results = 'hide'}

# formerly `strat`
unit_data <- read_csv(here::here('data', 'macrostrat_units.csv'))

# formerly `fossil`
collect_data <- read_csv(here::here('data', 'macrostrat_collect.csv'))

# formerly `taxon
pbdb_data <- read_csv(here::here('data', 'pbdb_occurrences.csv'))

```



## Joining Macrostrat and PBDB

Now that that's out of the way, let's start getting out data into shape. An obvious first step is connect the Macrostrat units table to the fossils table. 

There are multiple collections occurrences per unit, so I'll add all the appropriate unit information to each fossil collection. I'm not worrying about units that lack fossils (yet), hence the left join. 

```{r join_macrostrat}

collect_unit <- left_join(x = collect_data, y = unit_data, 
                         by = c('unit_id', 'col_id', 't_age', 'b_age')) %>%
  dplyr::select(-(genus_no:taxon_no), refs.y) # redundant/unnecessary

print(collect_unit)

```

This gives a single table with all fossil collections associated with their geological information (lithology, age, etc.) The next step is associating the geological context of each collection with the individual occurrences within that collection. I'm only caring about occurrences recorded in the PBDB -- they have the most information -- hence the left join. I'm also going to filter the occurrences to just those of shelly marine taxa which I defined above as one of the useful constants.

```{r join_pbdb}

occur_collect <- 
  pbdb_data %>%
  filter(phylum %in% shelly | class %in% shelly) %>%
  left_join(., collect_unit, by = c('collection_no' = 'cltn_id')) %>%
  mutate(occur_mid_ma = (max_ma + min_ma) / 2,
         unit_mid_ma = (t_age + b_age) / 2)

```


## Filter by occurrence age

A big issue is that these occurrences from all over time, not just the Ordovician + Silurian. Just look.

```{r vis_ages}

occur_collect %>% 
  ggplot(aes(x = occur_mid_ma)) +
  geom_bar(width = 2) +
  scale_x_reverse()

```

This problem due to geological units not being restricted to a particular time interval. A unit can range through, into, or out of our target temporal range.

To overcome this and focus only on fossil occurrences from the Ordovician or Silurian, I filtering down to only those occurrences who's mid-point age is within `temp_range`.

```{r filter_age}

occur_collect %<>%
  filter(between(occur_mid_ma, temp_range[2], temp_range[1]))

occur_collect %>%
  ggplot(aes(x = occur_mid_ma)) +
  geom_bar(width = 2) +
  scale_x_reverse()

```

Much better.



## Binning occurrences

The fossil record is a discrete recording of continuous time which creates all kinds of problems. Additionally, paleo-time is more of a range than an instaneous value. Binning out data helps alleviate this -- it just requires us to define a temporal width for each bin. There are two ways to do this: predetermine how many bins you want, or pick an (arbitrary) value. Luckily I've written a function that can do both: `bin_ages()`.

I want to compare multiple arbitrary bin widths to pick whatever feels "smoothest". Sadly, I have trouble defining this algorithmically, but the general idea is to have as many bins as possible to have 1+ fossils and there are as few bins as possible with 0 fossils. Common bin widths include 1, 2, 5, and 10 million years, but I'm gonna look at a few more. 

```{r compare_bin}

occur_collect %>%
  transmute(bin_01 = bin_ages(occur_mid_ma, by = 1, age = TRUE),
            bin_02 = bin_ages(occur_mid_ma, by = 2, age = TRUE),
            bin_025 = bin_ages(occur_mid_ma, by = 2.5, age = TRUE),
            bin_05 = bin_ages(occur_mid_ma, by = 5, age = TRUE),
            bin_10 = bin_ages(occur_mid_ma, by = 10, age = TRUE)) %>%
  gather(key = key, value = value) %>%
  ggplot(aes(x = value)) +
  geom_bar(width = 2) +
  facet_grid(key ~ .) +
  scale_x_reverse()

```

2 and 2.5 seem like the nicest of these options, and I like to favor having a lot of bins over having few bins so I'm going to stick with 2 million year bins.


### Saving the Hirnantian

However, there is a twist -- we're *really* interested in the occurrence patterns during the Hirnantian. This means I'd like the Hirnantian time interval preserved. The Hirnantian lasted from 445.2 to 443.8 Ma -- a *really* small interval of time.

A solution is to bin all the data *before* and *after* the Hirnantian as per usual, leaving the Hirnantian as a one-off.  Here is that effort.


```{r bin_occur}

occur_before <- 
  occur_collect %>%
  filter(occur_mid_ma > max(hirnantian)) %>%
  mutate(bin_age = bin_ages(occur_mid_ma, by = 2, age = TRUE)) %>%
  dplyr::select(occurrence_no, bin_age)

occur_after <- 
  occur_collect %>%
  filter(occur_mid_ma < min(hirnantian)) %>%
  mutate(bin_age = bin_ages(occur_mid_ma, by = 2, age = TRUE)) %>%
  dplyr::select(occurrence_no, bin_age)


occur_collect %<>%
  left_join(., occur_before, by = 'occurrence_no') %>%
  left_join(., occur_after, by = 'occurrence_no') %>%
  mutate(bin_age = coalesce(bin_age.x, bin_age.y),
         bin_age = if_else(is.na(bin_age),
                           true = 445.2,
                           false = bin_age),
         # worst trick in the book
         # lower values, younger bin
         bin_num = as.numeric(as.factor(bin_age))) %>% 
  dplyr::select(-bin_age.x, -bin_age.y)

write_csv(occur_collect, here::here('data', 'occurrences_clean.csv'))

```

Now are tibble `occur_collect` has every observation assigned to a temporal bin and we know one of these bins (i.e. 445.2) is the Hirnantian. I've saved this to disk so it can be shared/interrogated on its own.
